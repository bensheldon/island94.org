---
title: "How I'm thinking about AI (LLMs)"
date: 2025-01-05 14:49 PST
published: true
tags: [ai, opinions]
---

With AI, in my context we‚Äôre talking about LLMs (Large Language Models), which I simplify down to ‚Äútext generator‚Äù: they take text as input, and they output text.

I wrote this to share with some folks I‚Äôm collaborating with on building AI-augmented workflows. I‚Äôve struggled to find something that is both condensed and whose opinionations match my own. So I wrote it myself.

The following explanation is intended to be accurate, but not particularly precise.  For example, there is ChatGPT the product, there is an LLM at the bottom, and then in the middle there are other functions and capabilities. Or Claude or AWS Nova or Llama. These things are more than _*just*_ LLMs, but they are also *not much* more than an LLM. Some of these tools can also interpret images and documents and audio and video. To do so, they‚Äôre passing those documents through specialized functions like OCR (optical character recognition), voice-recognition and image-recognition tools and then those results are turned into more text input. And some of them can take ‚ÄúActions‚Äù with ‚ÄúAgents‚Äù which is still based on text output, just being structured and fed into something else. It‚Äôs text text text.


(also, if something is particularly wrong, let me know please)

### A little about LLMs

The language around LLMs and ‚ÄúAI‚Äù is fucked up with hype and inappropriate metaphors. But the general idea is there is two key phases to keep track of:

1. Training: Baking the model. At which point it‚Äôs done. I don‚Äôt know anyone who is actually building models Everyone is using something like OpenAI or Claude or Llama. And even while these things can be ‚Äúfine tuned‚Äù I don‚Äôt know anyone doing it; operating at the model level requires input data on the order of tens of thousands of inputs/examples.
2. Prompting: Using the model, giving input and getting output. This is everything the vast majority of developers are doing.

That‚Äôs it. Those are the only two buckets you need to think about.

#### 1. Training

The way AI models get made is to first collect trillions of pages of written text (an example is [Common Crawl](https://commoncrawl.org/) which scrapes the Internet). Then use machine learning to identify probabilistic patterns that can be represented by only several billion variables (floating point numbers). This is called **‚ÄúPre Training‚Äù**. At this point, you can say ‚ÄúBased on the input data, it‚Äôs probabilistically likely that the word after ‚Äúeeny meany miney‚Äù is ‚Äúmoe‚Äù.

Then there is the phase of **‚ÄúFine Tuning‚Äù** which makes sure that longer strings of text input are completed in ways that are intended (never right or wrong, just intended or expected). For example, if the text input is ‚ÄúWrite me a Haiku about frogs‚Äù you expect a short haiku about frogs and not a treatise on the magic of the written word or amphibians. Fine tuning is largely accomplished by tens of thousands of workers in Africa and South Asia reading examples of inputs and outputs and clicking üëç or üëé on their screen. This is then fed back into machine learning models to say, of the billion variables, which variables should get a little more or less oomph when they‚Äôre calculating the output. Fine Tuning requires tens of thousands of these scored examples; again, this is probabilistic-scale stuff. This can also be called **RLHF (Reinforcement Learning from Human Feedback)**, though that sometimes also refers to few-shot prompting, which is Prompt-phase (‚ÄúLearning‚Äù is a nonsense word in the AI domain; it has zero salience without clarifying which phase you‚Äôre talking about). A lot of the interesting fine-tuning, imo, comes from getting these text generators to:

- Read like a human chatting with you, rather than textual diarrhea
- Getting structured output, like valid JSON, rather than textual diarrhea

Note: You can mentally slot in words like ‚Äúparameters‚Äù, ‚Äúdimensions‚Äù, ‚Äúweights‚Äù and ‚Äúlayers‚Äù into all this. Also whenever someone says ‚Äúwe don‚Äôt really know how they work‚Äù what they really mean is ‚Äúthere‚Äôs a lot of variables and I didn‚Äôt specifically look at them all‚Äù. But that‚Äôs no different than being given an Excel spreadsheet with several VLOOKUPS and functions and saying ‚Äúsure, that looks ok‚Äù and copy-pasting the report on to your boss; I mean, you _could_ figure it all out, but it seems to work and you‚Äôre a busy person.

Ok, now we‚Äôre done with training. The model at this point is baked and no further modification takes place: no memory, no storage, no ‚Äúlearning‚Äù in the sense of a biological process. From this point further they operate as a function: input in, output out, no side effects.

Here‚Äôs how AWS Bedrock, which is how I imagine lots of companies are using AI in their product, [describes all this](https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html):

> After delivery of a model from a model provider [Anthropic, OpenAI, Meta] to AWS, Amazon Bedrock will perform a deep copy of a model provider‚Äôs inference and training software into those accounts for deployment. Because the model providers don‚Äôt have access to those accounts, they don‚Äôt have access to Amazon Bedrock logs or to customer prompts and completions.

See! It‚Äôs all just dead artifacts uploaded into S3, that are then loaded onto EC2 on-demand. A fancy lambda! Nothing more.

#### 2. Prompting

Prompting is when we give the model input, and then it gives back some output. That‚Äôs it. Unless we are specifically collecting trillions of documents, or doing fine-tuning against thousands of examples (which we are NOT!), we are simply writing a prompt, and having the model generate some text based on it. It riffs. The output can be called ‚Äúcompletions‚Äù because they‚Äôre just that: More words.

(Fun fact: how to get the LLM to _stop_ writing words is a hard problem to solve)

Note: You might sometimes see prompting called model ‚Äútesting‚Äù (as opposed to model building or training). That‚Äôs because you‚Äôre powering up the artifact to put some words through it. Testing testing is called ‚ÄúEvaluations‚Äù (‚ÄúEvals‚Äù for short) and like all test test regimes the lukewarm debate I hear from everybody is ‚Äúwe aren‚Äôt but should we?‚Äù

### Writing prompts

This is the work! Unfortunately the language used to describe all of this is truly and totally fucked. By which I mean that words like ‚Äúlearning‚Äù and ‚Äúthought‚Äù and ‚Äúmemory‚Äù and even ‚Äútraining‚Äù is reused again.

It‚Äôs all about simply writing a prompt that boops the resulting text generator output into the shape you want. It‚Äôs all snoot-booping, all the time.

Going back to the Training data, let‚Äôs make some conceptual distinctions:

* Content: specific facts, statements and assertions that were (possibly) encoded into those billions of probabilities from the training data
* Structure: the overall probability that a string of words (really fragments of words) comes out again looking like something we expect, which has been adjusted via Fine Tuning

Remember, this is just a probabilistic text generator. So there is probabilistic facts, and probabilistic structure. And that probabilistic part is why we have words like ‚Äúhallucination‚Äù and ‚Äúslop‚Äù and ‚Äúsafety‚Äù. There‚Äôs no there there. It‚Äôs just probabilities. There‚Äôs no guarantee that a particular fact has been captured in those billions of variables. It‚Äôs just a text generator. And it‚Äôs been trained on a lot of dumb shit people write. It‚Äôs *just* a text generator. Don‚Äôt trust it.

So on to some prompting strategies:

* **Zero-Shot Prompting:** This just means to ask something open-ended and the AI returns something that probabilistical follows:
  > Classify the sentiment of the following review as positive, neutral, or negative: ‚ÄúThe quality is amazing, and it exceeded my expectations‚Äù
* **Few-Shot (or one-shot/multi-shot) Prompting**: This just means to provide one or more examples of ‚Äúexpected‚Äù completions in the prompt (remember, this is all prompt, not fine-tuning) to try to narrow down what could probabilistically follow:
  > Task: Classify the sentiment of the following reviews as positive, neutral, or negative.
  > Examples:
  > 1. ‚ÄúI absolutely adore this product. It‚Äôs fantastic!‚Äù - positive
  > 2. ‚ÄúIt‚Äôs okay, not the best I‚Äôve used.‚Äù - neutral
  > 3. ‚ÄúThis is terrible. I regret buying it.‚Äù - negative
  > Now classify this review:
  > 4. ‚ÄúThe quality is amazing, and it exceed my expectations‚Äù - [it‚Äôs blank, for the model to finish]

*Note: Zero/One/Few/Multi-Shot is sometimes called ‚ÄúLearning‚Äù instead of ‚ÄúPrompting‚Äù. This is a terrible name, because there is no learning (the models are dead!) but is one of those things where the most assume-good-intent explanation is that over the course of the prompt and its incrementally generated completion that the output assumes the desired shape.*

- **Chain of Thought Prompting**: The idea here is that the prompt includes a description of how a human might explain what they were doing to complete the prompt. And that boops the completion into filling out those steps, and arriving at a more expected answer:
  > Classify the sentiment of the following review as positive, neutral, or negative.
  > ‚ÄúI absolutely adore this product. It‚Äôs fantastic!‚Äù
  > Analysis 1: How does it describe the product?
  > Analysis 2: How does it describe the functionality of the product?
  > Analysis 3: How does it describe their relationship with the product?
  > Analysis 4: How does it describe how friends, family, or others relate to the product?
  > Overall: Is it positive, neutral, or negative?

*Note: again, there is no ‚Äúthought‚Äù happening. The point of the prompt is to boop the text completion into giving a more expected answer. There are some new models (as of late 2024) that are supposed to do Chain-of-Thought implicitly; afaik there is just a hidden/unshown prompt that says ‚Äúbreak this down into steps‚Äù and an intermediate output that takes the output of that and feeds it into another hidden/unshown prompt and then the output of that is shown to you. That‚Äôs why they costs more, cause it‚Äôs invoking the the LLM twice on your behalf.*

* **Chain Prompting**: This simply means that you take the output of one prompt, and then feed that into a new prompt. This can be useful to isolate a specific prompt and output. It might also be necessary because of the length: LLMs can only operate on so many words, so if you need to summarize a long document in a prompt, you‚Äôd need to first break it down into smaller chunks, use the LLM to summarize each chunk, and then combine the summaries into a new prompt for the LLM summarize that.
* **RAG (Retrieval Augmented Generation) Prompting**: This means that you look up some info in a database, and then insert that into the prompt before handing it to the LLM. Everything is prompt, there is only prompt.

*Note: **‚ÄúEmbeddings‚Äù** are a way of search indexing your text. LLMs take all those trillions of documents and probabilistically boils them down to several billion variables. Embeddings boil down further to a couple thousand variables (floating point numbers). Creating an embedding means providing a piece of text, and you get back the values of those thousand floating point numbers that probabilistically describe that text (big brain idea: it is the document‚Äôs location in a thousand-dimensional space). That lets you compute across multiple documents ‚Äúgiven this document within the n-dimensional space, what are its closest neighboring documents semantically/probabilistically?‚Äù Embeddings are useful when you want to do RAG Prompting to pull out relevant documents and insert their text into your prompt before it‚Äôs fed to the LLM to generate output.*

- **Cues and Nudges** There are [certain phrases](https://news.ycombinator.com/item?id=40474716), like ‚Äúno yapping‚Äù or [‚Äútake a deep breath‚Äù](https://arstechnica.com/information-technology/2023/09/telling-ai-model-to-take-a-deep-breath-causes-math-scores-to-soar-in-study/) that change the output. I don‚Äôt think there is anything delightful about this; it‚Äôs simply trying to boop up the variables you want in your output and words are the only inputs you have. I‚Äôm sure there will someday be better ways to do it, but whatever works.

### A strong opinion about zero-shot prompting

Don‚Äôt do it! I think it‚Äôs totally fine if you just want to ask a question and try to intuit the extent that the model has been trained or tuned on the particular domain you‚Äôre curious about. But you should put ZERO stock in the answer as something factual.

If you need facts, you must provide the facts as part of your prompt. That means:

- Providing a giant pile of text as content, or breaking it down (like via embeddings) and injecting smaller chunks via RAG
- Providing any and all input you ever expect to get out of the output

It‚Äôs ok to summarize, extract, translate or sentiment. The only reason it‚Äôs ok to zero-shot code is because it‚Äôs machine verifiable (you run it). Otherwise, you must verify! Or don‚Äôt do it at all.
