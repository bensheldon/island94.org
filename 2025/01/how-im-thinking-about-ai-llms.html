<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>How I&#39;m thinking about AI (LLMs) | Island94.org</title>

  <meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="b-mwq7aabkHSZUtFcgHZNGW4gdwi9QX1Qw01QGJoz6t2TUjsLOqAs_3mv386OIrbEG3XR5aS9W6kg6QBHml4-g" />
  

  <link rel="alternate" type="application/rss+xml" title="Island94.org" href="https://island94.org/feed.xml">

  <link rel="stylesheet" href="/assets/application-2d0874f724227ac5e099df0bd7cfeb3a4fc86f9358ede0d8216b193fa7a7da99.css" data-turbo-track="reload" />
  <script src="/assets/main-57967bd29e3a27ba3099fee5997ee6ec8c59ef209085f949b6eac799b21915b0.js"></script>

  <script>
    let theme = localStorage.getItem('theme');
    if (!["light", "dark"].includes(theme)) {
      theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
    }
    document.documentElement.setAttribute('data-bs-theme', theme);
  </script>

  
</head>
<body>
<nav class="navbar navbar-expand-lg navbar-light mb-2 flex-column">
  <div class="container">
    <a class="navbar-brand" href="/">Island94.org</a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
        <li class="nav-item"><a class="nav-link" href="/about">About</a></li>
        <li class="nav-item"><a class="nav-link" href="/archives">Archives</a></li>
        <li class="nav-item"><a class="nav-link" href="/tags">Tags</a></li>
        <li class="nav-item"><a class="nav-link" href="/feed.xml">RSS Feed <i class="bi bi-rss-fill"></i></a></li>

        <li class="nav-item py-2 py-lg-1 col-12 col-lg-auto">
          <div class="vr d-none d-lg-flex h-100 mx-lg-2 text-light-500"></div>
          <hr class="d-lg-none my-0 text-light-500">
        </li>

        <li class="nav-item dropdown">
          <button id="bs-theme" class="btn btn-link nav-link dropdown-toggle" type="button" aria-expanded="false" data-bs-toggle="dropdown"  aria-label="Theme (Auto)">
            <i data-bs-theme-active-icon class="bi bi-circle-half"></i>
            <span data-bs-theme-label>Theme</span>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bs-theme-text">
            <li>
              <button type="button" class="dropdown-item" data-bs-theme-value="light" aria-pressed="false">
                <i class="bi bi-sun"></i> Light
              </button>
            </li>
            <li>
              <button type="button" class="dropdown-item" data-bs-theme-value="dark" aria-pressed="false">
                <i class="bi bi-moon-stars-fill"></i> Dark
              </button>
            </li>
            <li>
              <button type="button" class="dropdown-item" data-bs-theme-value="auto" aria-pressed="true">
                <i class="bi bi-circle-half"></i> Auto
              </button>
            </li>
          </ul>
        </li>

      </ul>

      <form id="search" class="d-flex" action="/search" method="get">
        <input id="search-box" class="form-control search" role="search" name="q" placeholder="Search" aria-label="Search" results="0" type="text">
        <button class="visually-hidden" type="submit">Search</button>
      </form>
    </div>
  </div>
  <div class="container">
    <div class="text-muted fst-italic">
      â€” Ben Sheldon's personal blog and <a href="/2012/04/a-commonplace-book">commonplace book</a>.
    </div>
  </div>
</nav>


  <div class="container container-body">
    
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">
      <a class="post-link" href="/2025/01/how-im-thinking-about-ai-llms"><p>How Iâ€™m thinking about AI (LLMs)</p>
</a>
    </h1>
    <p class="post-meta text-muted">
        <time datetime="2025-01-05T14:49:00-08:00" itemprop="datePublished">January 5, 2025</time>

        â€¢ Tagged <a href="/posts/tags/ai">ai</a> and <a href="/posts/tags/opinions">opinions</a>
    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>With AI, in my context weâ€™re talking about LLMs (Large Language Models), which I simplify down to â€œtext generatorâ€: they take text as input, and they output text.</p>

<p>I wrote this to share with some folks Iâ€™m collaborating with on building AI-augmented workflows. Iâ€™ve struggled to find something that is both condensed and whose opinionations match my own. So I wrote it myself.</p>

<p>The following explanation is intended to be accurate, but not particularly precise.  For example, there is ChatGPT the product, there is an LLM at the bottom, and then in the middle there are other functions and capabilities. Or Claude or AWS Nova or Llama. These things are more than <em>*just*</em> LLMs, but they are also <em>not much</em> more than an LLM. Some of these tools can also interpret images and documents and audio and video. To do so, theyâ€™re passing those documents through specialized functions like OCR (optical character recognition), voice-recognition and image-recognition tools and then those results are turned into more text input. And some of them can take â€œActionsâ€ with â€œAgentsâ€ which is still based on text output, just being structured and fed into something else. Itâ€™s text text text.</p>

<p>(also, if something is particularly wrong, let me know please)</p>

<h3 id="a-little-about-llms">A little about LLMs</h3>

<p>The language around LLMs and â€œAIâ€ is fucked up with hype and inappropriate metaphors. But the general idea is there is two key phases to keep track of:</p>

<ol>
  <li>Training: Baking the model. At which point itâ€™s done. I donâ€™t know anyone who is actually building models Everyone is using something like OpenAI or Claude or Llama. And even while these things can be â€œfine tunedâ€ I donâ€™t know anyone doing it; operating at the model level requires input data on the order of tens of thousands of inputs/examples.</li>
  <li>Prompting: Using the model, giving input and getting output. This is everything the vast majority of developers are doing.</li>
</ol>

<p>Thatâ€™s it. Those are the only two buckets you need to think about.</p>

<h4 id="1-training">1. Training</h4>

<p>The way AI models get made is to first collect trillions of pages of written text (an example is <a href="https://commoncrawl.org/">Common Crawl</a> which scrapes the Internet). Then use machine learning to identify probabilistic patterns that can be represented by only several billion variables (floating point numbers). This is called <strong>â€œPre Trainingâ€</strong>. At this point, you can say â€œBased on the input data, itâ€™s probabilistically likely that the word after â€œeeny meany mineyâ€ is â€œmoeâ€.</p>

<p>Then there is the phase of <strong>â€œFine Tuningâ€</strong> which makes sure that longer strings of text input are completed in ways that are intended (never right or wrong, just intended or expected). For example, if the text input is â€œWrite me a Haiku about frogsâ€ you expect a short haiku about frogs and not a treatise on the magic of the written word or amphibians. Fine tuning is largely accomplished by tens of thousands of workers in Africa and South Asia reading examples of inputs and outputs and clicking ğŸ‘ or ğŸ‘ on their screen. This is then fed back into machine learning models to say, of the billion variables, which variables should get a little more or less oomph when theyâ€™re calculating the output. Fine Tuning requires tens of thousands of these scored examples; again, this is probabilistic-scale stuff. This can also be called <strong>RLHF (Reinforcement Learning from Human Feedback)</strong>, though that sometimes also refers to few-shot prompting, which is Prompt-phase (â€œLearningâ€ is a nonsense word in the AI domain; it has zero salience without clarifying which phase youâ€™re talking about). A lot of the interesting fine-tuning, imo, comes from getting these text generators to:</p>

<ul>
  <li>Read like a human chatting with you, rather than textual diarrhea</li>
  <li>Getting structured output, like valid JSON, rather than textual diarrhea</li>
</ul>

<p>Note: You can mentally slot in words like â€œparametersâ€, â€œdimensionsâ€, â€œweightsâ€ and â€œlayersâ€ into all this. Also whenever someone says â€œwe donâ€™t really know how they workâ€ what they really mean is â€œthereâ€™s a lot of variables and I didnâ€™t specifically look at them allâ€. But thatâ€™s no different than being given an Excel spreadsheet with several VLOOKUPS and functions and saying â€œsure, that looks okâ€ and copy-pasting the report on to your boss; I mean, you <em>could</em> figure it all out, but it seems to work and youâ€™re a busy person.</p>

<p>Ok, now weâ€™re done with training. The model at this point is baked and no further modification takes place: no memory, no storage, no â€œlearningâ€ in the sense of a biological process. From this point further they operate as a function: input in, output out, no side effects.</p>

<p>Hereâ€™s how AWS Bedrock, which is how I imagine lots of companies are using AI in their product, <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/data-protection.html">describes all this</a>:</p>

<blockquote>
  <p>After delivery of a model from a model provider [Anthropic, OpenAI, Meta] to AWS, Amazon Bedrock will perform a deep copy of a model providerâ€™s inference and training software into those accounts for deployment. Because the model providers donâ€™t have access to those accounts, they donâ€™t have access to Amazon Bedrock logs or to customer prompts and completions.</p>
</blockquote>

<p>See! Itâ€™s all just dead artifacts uploaded into S3, that are then loaded onto EC2 on-demand. A fancy lambda! Nothing more.</p>

<h4 id="2-prompting">2. Prompting</h4>

<p>Prompting is when we give the model input, and then it gives back some output. Thatâ€™s it. Unless we are specifically collecting trillions of documents, or doing fine-tuning against thousands of examples (which we are NOT!), we are simply writing a prompt, and having the model generate some text based on it. It riffs. The output can be called â€œcompletionsâ€ because theyâ€™re just that: More words.</p>

<p>(Fun fact: how to get the LLM to <em>stop</em> writing words is a hard problem to solve)</p>

<p>Note: You might sometimes see prompting called model â€œtestingâ€ (as opposed to model building or training). Thatâ€™s because youâ€™re powering up the artifact to put some words through it. Testing testing is called â€œEvaluationsâ€ (â€œEvalsâ€ for short) and like all test test regimes the lukewarm debate I hear from everybody is â€œwe arenâ€™t but should we?â€</p>

<h3 id="writing-prompts">Writing prompts</h3>

<p>This is the work! Unfortunately the language used to describe all of this is truly and totally fucked. By which I mean that words like â€œlearningâ€ and â€œthoughtâ€ and â€œmemoryâ€ and even â€œtrainingâ€ is reused again.</p>

<p>Itâ€™s all about simply writing a prompt that boops the resulting text generator output into the shape you want. Itâ€™s all snoot-booping, all the time.</p>

<p>Going back to the Training data, letâ€™s make some conceptual distinctions:</p>

<ul>
  <li>Content: specific facts, statements and assertions that were (possibly) encoded into those billions of probabilities from the training data</li>
  <li>Structure: the overall probability that a string of words (really fragments of words) comes out again looking like something we expect, which has been adjusted via Fine Tuning</li>
</ul>

<p>Remember, this is just a probabilistic text generator. So there is probabilistic facts, and probabilistic structure. And that probabilistic part is why we have words like â€œhallucinationâ€ and â€œslopâ€ and â€œsafetyâ€. Thereâ€™s no there there. Itâ€™s just probabilities. Thereâ€™s no guarantee that a particular fact has been captured in those billions of variables. Itâ€™s just a text generator. And itâ€™s been trained on a lot of dumb shit people write. Itâ€™s <em>just</em> a text generator. Donâ€™t trust it.</p>

<p>So on to some prompting strategies:</p>

<ul>
  <li><strong>Zero-Shot Prompting:</strong> This just means to ask something open-ended and the AI returns something that probabilistical follows:
    <blockquote>
      <p>Classify the sentiment of the following review as positive, neutral, or negative: â€œThe quality is amazing, and it exceeded my expectationsâ€</p>
    </blockquote>
  </li>
  <li><strong>Few-Shot (or one-shot/multi-shot) Prompting</strong>: This just means to provide one or more examples of â€œexpectedâ€ completions in the prompt (remember, this is all prompt, not fine-tuning) to try to narrow down what could probabilistically follow:
    <blockquote>
      <p>Task: Classify the sentiment of the following reviews as positive, neutral, or negative.<br />
Examples:</p>
      <ol>
        <li>â€œI absolutely adore this product. Itâ€™s fantastic!â€ - positive</li>
        <li>â€œItâ€™s okay, not the best Iâ€™ve used.â€ - neutral</li>
        <li>â€œThis is terrible. I regret buying it.â€ - negative<br />
Now classify this review:</li>
        <li>â€œThe quality is amazing, and it exceed my expectationsâ€ - [itâ€™s blank, for the model to finish]</li>
      </ol>
    </blockquote>
  </li>
</ul>

<p><em>Note: Zero/One/Few/Multi-Shot is sometimes called â€œLearningâ€ instead of â€œPromptingâ€. This is a terrible name, because there is no learning (the models are dead!) but is one of those things where the most assume-good-intent explanation is that over the course of the prompt and its incrementally generated completion that the output assumes the desired shape.</em></p>

<ul>
  <li><strong>Chain of Thought Prompting</strong>: The idea here is that the prompt includes a description of how a human might explain what they were doing to complete the prompt. And that boops the completion into filling out those steps, and arriving at a more expected answer:
    <blockquote>
      <p>Classify the sentiment of the following review as positive, neutral, or negative.<br />
â€œI absolutely adore this product. Itâ€™s fantastic!â€<br />
Analysis 1: How does it describe the product?<br />
Analysis 2: How does it describe the functionality of the product?<br />
Analysis 3: How does it describe their relationship with the product?<br />
Analysis 4: How does it describe how friends, family, or others relate to the product?<br />
Overall: Is it positive, neutral, or negative?</p>
    </blockquote>
  </li>
</ul>

<p><em>Note: again, there is no â€œthoughtâ€ happening. The point of the prompt is to boop the text completion into giving a more expected answer. There are some new models (as of late 2024) that are supposed to do Chain-of-Thought implicitly; afaik there is just a hidden/unshown prompt that says â€œbreak this down into stepsâ€ and an intermediate output that takes the output of that and feeds it into another hidden/unshown prompt and then the output of that is shown to you. Thatâ€™s why they costs more, cause itâ€™s invoking the the LLM twice on your behalf.</em></p>

<ul>
  <li><strong>Chain Prompting</strong>: This simply means that you take the output of one prompt, and then feed that into a new prompt. This can be useful to isolate a specific prompt and output. It might also be necessary because of the length: LLMs can only operate on so many words, so if you need to summarize a long document in a prompt, youâ€™d need to first break it down into smaller chunks, use the LLM to summarize each chunk, and then combine the summaries into a new prompt for the LLM summarize that.</li>
  <li><strong>RAG (Retrieval Augmented Generation) Prompting</strong>: This means that you look up some info in a database, and then insert that into the prompt before handing it to the LLM. Everything is prompt, there is only prompt.</li>
</ul>

<p><em>Note: <strong>â€œEmbeddingsâ€</strong> are a way of search indexing your text. LLMs take all those trillions of documents and probabilistically boils them down to several billion variables. Embeddings boil down further to a couple thousand variables (floating point numbers). Creating an embedding means providing a piece of text, and you get back the values of those thousand floating point numbers that probabilistically describe that text (big brain idea: it is the documentâ€™s location in a thousand-dimensional space). That lets you compute across multiple documents â€œgiven this document within the n-dimensional space, what are its closest neighboring documents semantically/probabilistically?â€ Embeddings are useful when you want to do RAG Prompting to pull out relevant documents and insert their text into your prompt before itâ€™s fed to the LLM to generate output.</em></p>

<ul>
  <li><strong>Cues and Nudges</strong> There are <a href="https://news.ycombinator.com/item?id=40474716">certain phrases</a>, like â€œno yappingâ€ or <a href="https://arstechnica.com/information-technology/2023/09/telling-ai-model-to-take-a-deep-breath-causes-math-scores-to-soar-in-study/">â€œtake a deep breathâ€</a> that change the output. I donâ€™t think there is anything delightful about this; itâ€™s simply trying to boop up the variables you want in your output and words are the only inputs you have. Iâ€™m sure there will someday be better ways to do it, but whatever works.</li>
</ul>

<h3 id="a-strong-opinion-about-zero-shot-prompting">A strong opinion about zero-shot prompting</h3>

<p>Donâ€™t do it! I think itâ€™s totally fine if you just want to ask a question and try to intuit the extent that the model has been trained or tuned on the particular domain youâ€™re curious about. But you should put ZERO stock in the answer as something factual.</p>

<p>If you need facts, you must provide the facts as part of your prompt. That means:</p>

<ul>
  <li>Providing a giant pile of text as content, or breaking it down (like via embeddings) and injecting smaller chunks via RAG</li>
  <li>Providing any and all input you ever expect to get out of the output</li>
</ul>

<p>Itâ€™s ok to summarize, extract, translate or sentiment. The only reason itâ€™s ok to zero-shot code is because itâ€™s machine verifiable (you run it). Otherwise, you must verify! Or donâ€™t do it at all.</p>

  </div>
</article>


<div class="card my-4">
  <div class="card-body">
    Discuss this with me on <a href="https://twitter.com/bensheldon"><i class="bi bi-twitter"></i> Twitter</a>
    or suggest a
    <a href="https://github.com/bensheldon/island94.org/blob/main/_posts/2025-01-05-how-im-thinking-about-ai-llms.md"><i class="bi bi-github"></i>
      change</a>.
  </div>
  </div>
</div>

  </div>

</body>
</html>
